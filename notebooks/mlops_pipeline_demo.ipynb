{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6e1a0c",
   "metadata": {},
   "source": [
    "# MLOps Production Pipeline - Data Exploration and Model Training\n",
    "\n",
    "This notebook demonstrates the complete MLOps pipeline including data loading, feature engineering, model training, and deployment preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce7053",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing packages\n",
    "%pip install matplotlib\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Custom modules\n",
    "from data.data_loader import DataLoader, DataPreprocessor\n",
    "from features.feature_engineering import FeatureEngineer, FeatureScaler, FeatureSelector\n",
    "from deployment.deployment_utils import ModelDeployment, create_model_artifact\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65456c1d",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(\"../data\")\n",
    "\n",
    "# Load or generate sample data\n",
    "df = data_loader.generate_sample_data(n_samples=2000)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Description:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Target Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Target Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target distribution:\\n{df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe60c8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc408247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Clean data\n",
    "df_clean = preprocessor.clean_data(df)\n",
    "print(f\"Data shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "df_encoded = preprocessor.encode_categorical(df_clean)\n",
    "print(f\"Data shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Split features and target\n",
    "X, y = preprocessor.split_features_target(df_encoded, 'target')\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d31d8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Create interaction features\n",
    "X_interactions = engineer.create_interaction_features(X)\n",
    "print(f\"Shape after interaction features: {X_interactions.shape}\")\n",
    "\n",
    "# Create polynomial features\n",
    "X_poly = engineer.create_polynomial_features(X_interactions, degree=2)\n",
    "print(f\"Shape after polynomial features: {X_poly.shape}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = FeatureScaler(method='standard')\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "print(f\"Shape after scaling: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44650d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "selector = FeatureSelector(method='univariate', k=15)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "print(f\"Shape after feature selection: {X_selected.shape}\")\n",
    "print(f\"Selected features: {selector.selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544140b",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20033db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aebd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_selected.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217a055",
   "metadata": {},
   "source": [
    "## 6. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metrics for model artifact\n",
    "metrics = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'n_features': X_selected.shape[1],\n",
    "    'n_samples_train': X_train.shape[0],\n",
    "    'n_samples_test': X_test.shape[0]\n",
    "}\n",
    "\n",
    "# Create model artifact\n",
    "artifact = create_model_artifact(\n",
    "    model=model,\n",
    "    model_name=\"random_forest_classifier\",\n",
    "    version=\"1.0\",\n",
    "    metrics=metrics,\n",
    "    feature_names=X_selected.columns.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Model artifact created: {artifact['model_filename']}\")\n",
    "print(f\"Model metadata: {artifact['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deployment\n",
    "deployment = ModelDeployment(\"../models\")\n",
    "deployment.load_model(artifact['model_filename'])\n",
    "\n",
    "# Test prediction\n",
    "sample_features = X_test.iloc[0].values\n",
    "prediction = deployment.predict(sample_features)\n",
    "probabilities = deployment.predict_proba(sample_features)\n",
    "\n",
    "print(f\"Sample prediction: {prediction[0]}\")\n",
    "print(f\"Prediction probabilities: {probabilities[0]}\")\n",
    "print(f\"Actual value: {y_test.iloc[0]}\")\n",
    "\n",
    "# Get model info\n",
    "model_info = deployment.get_model_info()\n",
    "print(f\"\\nModel Info: {model_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7787f75",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488efd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for future use\n",
    "processed_data = X_selected.copy()\n",
    "processed_data['target'] = y\n",
    "\n",
    "data_loader.save_processed_data(processed_data, \"processed_features.csv\")\n",
    "print(\"Processed data saved successfully!\")\n",
    "\n",
    "# Save reference data for monitoring\n",
    "reference_data = X_selected.sample(200, random_state=42)\n",
    "reference_data.to_csv(\"../data/reference/reference_features.csv\", index=False)\n",
    "print(\"Reference data saved for monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0aef7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete MLOps pipeline:\n",
    "\n",
    "1. **Data Loading**: Generated sample data and loaded it using our custom data loader\n",
    "2. **Data Preprocessing**: Cleaned and encoded the data\n",
    "3. **Feature Engineering**: Created interaction and polynomial features\n",
    "4. **Feature Selection**: Selected the most important features\n",
    "5. **Model Training**: Trained a Random Forest classifier\n",
    "6. **Model Evaluation**: Evaluated model performance\n",
    "7. **Model Deployment**: Prepared the model for deployment with metadata\n",
    "8. **Data Persistence**: Saved processed data for future use\n",
    "\n",
    "The trained model is now ready for deployment using the FastAPI service!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
